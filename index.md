# Workshop Summary

Learning-based methods, and in particular deep neural networks, have emerged as highly successful and universal tools for image and signal recovery and restoration. They achieve state-of-the-art results on tasks ranging from image denoising, image compression, and image reconstruction from few and noisy measurements. They are starting to be used in important imaging technologies, for example in GEs newest computational tomography scanners and in the newest generation of the iPhone.

The field has a range of theoretical and practical questions that remain unanswered. In particular, learning and neural network-based approaches often lack the guarantees of traditional physics-based methods. Further, while superior on average, learning-based methods can make drastic reconstruction errors, such as hallucinating a tumor in an MRI reconstruction or turning a pixelated picture of Obama into a white male.

This virtual workshop aims at bringing together theoreticians and practitioners in order to chart out recent advances and discuss new directions in deep neural network-based approaches for solving inverse problems in the imaging sciences and beyond.


# Schedule

The workshop schedule is aligned with 7:30 AM to 4 PM PT; please see this [converter](https://www.timeanddate.com/worldclock/fixedtime.html?msg=Deep+Inverse+Workshop&iso=20201211T0730&p1=256&ah=8&am=30) for conversion to your specific time zone.


| Time | Event |
| --- | --- |
| 7:30 | Newcomer presentation |
| 7:55 | Opening Remarks |
| 8:00 | Victor Lempinsky: *TBD* |
| 8:30 | Thomas Pock: *TBD* |
| 9:00 | Contributed talk 1 and Q&A |
| 9:15 | Contributed talk 2 and Q&A |
| 9:30 | Break and Discussion with Victor Lempinsky, Thomas Pock, and Erich Kobler |
| 10:00 | Rebecca Willett: *TBD* |
| 10:30 | Stefano Emron: *TBD* |
| 11:00 | Contributed talk 3 and Q&A |
| 11:15 | Contributed talk 4 and Q&A |
| 11:30 | Discussion with Rebecca Willett and Stefano Emron |
| 12:00 | Break  |
| 1:00 |  Poster session |
| 2:00 |  Peyman Milanfar |
| 2:30 | Rachel Ward |
| 3:00 | Larry Zitnick |
| 3:30 | Discussion with Peyman Milanfar, Rachel Ward, and Larry Zitnick |
| 4:00 | End of official program |




# Confirmed Speakers

- [Stefano Emron](https://cs.stanford.edu/~ermon/) (Stanford)
- [Victor Lempitsky](http://sites.skoltech.ru/compvision/members/vilem/) (Skoltech)
- [Peyman Milanfar](https://sites.google.com/view/milanfarhome/) (Google)
- [Thomas Pock](https://www.tugraz.at/institute/icg/research/team-pock/people/pock/) (TU Graz)
- [Rachel Ward](https://www.oden.utexas.edu/people/1143/) (UT Austin)
- [Rebecca Willett](https://voices.uchicago.edu/willett/) (University of Chicago)
- [Larry Zitnick](http://larryzitnick.org/) (Facebook AI Reseach)


# Call for Papers and Submission Instructions

We invite researchers to submit anonymous papers of up to 4 pages (excluding references and appendices) which will be considered for contributed workshop papers. No specific formatting is required. Authors are encouraged to use the [NeurIPS style file](https://nips.cc/Conferences/2020/PaperInformation/StyleFiles), but they may use any other style as long as it has standard font size (11pt) and margins (1in). The paper can have an appendix of unlimited lenght.

Submission at [OpenReview](https://openreview.net/group?id=NeurIPS.cc/2020/Workshop/Deep_Inverse) will be open from Sep. 1 until the submission deadline on October 9, 2020.

We welcome all submission in the intersection of inverse problems and deep learning including contributions related to robustness and biases, neural network architectures, regularization, optimization methods, datasets, theoretical foundations (including rigorous recovery guarantees, provable convergence, and bounds on representation errors),  untrained methods, generative models, end-to-end methods, and applications in imaging, time series, and beyond. We especially encourage submissions related to the following questions:

- Deep learning based approaches can make drastic reconstruction errors and may introduce biases. How common are such issues, can such reconstruction difficulties be alleviated, and if yes, how? 

- Deep learning based approaches often lack the guarantees of the traditional physics based methods. What theoretical results are necessary and possible?

- Untrained neural networks such as the deep image prior have shown that neural networks alone, without any learning, can give excellent reconstruction performance. How important is training on the target distribution for imaging performance and is it possible to achieve state-of-the art performance without training?

# Important Dates
- Submission Deadline: Extended to October 9, 2020.
- Notification: October 23, 2020.
- Workshop: Friday December 11

# Organizers
- [Reinhard Heckel](http://www.reinhardheckel.com/) (TUM)
- [Paul Hand](http://khoury.northeastern.edu/home/hand/) (Northeastern)
- [Soheil Feizi](https://www.cs.umd.edu/~sfeizi/) (UMD)
- [Lenka Zdeborova](http://artax.karlin.mff.cuni.cz/~zdebl9am/) (CEA/SACLAY)
- [Richard Baraniuk](http://richb.rice.edu/) (Rice University)

Please email [deepinverse@gmail.com](mailto:deepinverse@gmail.com) with any questions.
