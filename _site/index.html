<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Learning and Inverse Problems | Your awesome title</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Deep Learning and Inverse Problems" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="NeurIPS 2023 workshop, Saturday December 16, New Orleans" />
<meta property="og:description" content="NeurIPS 2023 workshop, Saturday December 16, New Orleans" />
<link rel="canonical" href="/" />
<meta property="og:url" content="/" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Learning and Inverse Problems" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"NeurIPS 2023 workshop, Saturday December 16, New Orleans","headline":"Deep Learning and Inverse Problems","name":"Your awesome title","url":"/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Deep Learning and Inverse Problems</h1>
      <h2 class="project-tagline">NeurIPS 2023 workshop, Saturday December 16, New Orleans</h2>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="workshop-description">Workshop Description</h1>

<p>Inverse problems are ubiquitous in science, medicine, and engineering, and research in this area has produced real-world impact in medical tomography, seismic imaging, computational photography, and other domains. The recent rapid progress in learning-based image generation raises exciting opportunities in inverse problems, and this workshop seeks to gather a diverse set of participants who apply machine learning to inverse problems, from mathematicians and computer scientists to physicists and biologists. This gathering will facilitate new collaborations and will help develop more effective, reliable, and trustworthy learning-based solutions to inverse problems.</p>

<h1 id="schedule">Schedule</h1>

<table>
  <thead>
    <tr>
      <th>Time (CDT)</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>9:00</td>
      <td>Invited talk 1: <a href="https://www.ece.iastate.edu/~namrata/">Namrata Vaswani</a></td>
    </tr>
    <tr>
      <td>9:30</td>
      <td>Invited talk 2: <a href="https://profiles.stanford.edu/266863">Todd Coleman</a></td>
    </tr>
    <tr>
      <td>10:00</td>
      <td>Contributed talks 1+2</td>
    </tr>
    <tr>
      <td>10:30</td>
      <td>Break and Discussion 1</td>
    </tr>
    <tr>
      <td>11:00</td>
      <td>Invited talk 3: <a href="https://bispl.weebly.com/professor.html">Jong Chul Ye</a></td>
    </tr>
    <tr>
      <td>11:30</td>
      <td>Invited talk 4: <a href="https://cs.stanford.edu/~poole/">Ben Poole</a></td>
    </tr>
    <tr>
      <td>12:00</td>
      <td>Contributed talks 3+4</td>
    </tr>
    <tr>
      <td>12:30</td>
      <td>Lunch break</td>
    </tr>
    <tr>
      <td>1:30</td>
      <td>Poster session and Discussion 2</td>
    </tr>
    <tr>
      <td>3:00</td>
      <td>Break</td>
    </tr>
    <tr>
      <td>3:30</td>
      <td>Invited talk 5: <a href="https://people.eecs.berkeley.edu/~kannanr/">Kannan Ramchandran</a></td>
    </tr>
    <tr>
      <td>4:00</td>
      <td>Invited talk 6: <a href="https://www.cs.princeton.edu/people/profile/zhonge">Ellen Zhong</a></td>
    </tr>
    <tr>
      <td>4:30</td>
      <td>Contributed talks 5 + 6</td>
    </tr>
    <tr>
      <td>5:00</td>
      <td>Discussion 3  and closing remarks</td>
    </tr>
    <tr>
      <td>5:30</td>
      <td>End of official program</td>
    </tr>
  </tbody>
</table>

<h1 id="speakers">Speakers</h1>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/Namrata.jpg" alt="Namrata Vaswani" width="225" /></td>
      <td><img src="/assets/images/ellen.jpg" alt="Ellen Zhong" width="225" /></td>
      <td><img src="/assets/images/jongchulye.png" alt="Jong Chul Ye" width="225" /></td>
    </tr>
    <tr>
      <td><a href="https://www.ece.iastate.edu/~namrata/">Namrata Vaswani</a><br />Iowa State University</td>
      <td><a href="https://www.cs.princeton.edu/people/profile/zhonge">Ellen Zhong</a><br />Princeton University</td>
      <td><a href="https://bispl.weebly.com/professor.html">Jong Chul Ye</a><br />KAIST</td>
    </tr>
    <tr>
      <td><img src="/assets/images/todd.jpg" alt="Todd Coleman" width="225" /></td>
      <td><img src="/assets/images/ben.png" alt="Ben Poole" width="225" /></td>
      <td><img src="/assets/images/kannan_crop.jpg" alt="Kannan Ramchandran" width="225" /></td>
    </tr>
    <tr>
      <td><a href="https://profiles.stanford.edu/266863">Todd Coleman</a><br />Stanford University</td>
      <td><a href="https://cs.stanford.edu/~poole/">Ben Poole</a><br />Google Brain</td>
      <td><a href="https://people.eecs.berkeley.edu/~kannanr/">Kannan Ramchandran</a><br />UC Berkeley</td>
    </tr>
  </tbody>
</table>

<h1 id="organizers">Organizers</h1>

<table>
  <tbody>
    <tr>
      <td><img src="/assets/images/Chris.jpg" alt="Christopher Metzler" width="225" /></td>
      <td><img src="/assets/images/Shirin.jpg" alt="Shirin Jalali" width="225" /></td>
      <td><img src="/assets/images/ajil_crop.jpg" alt="Ajil Jalal" width="225" /></td>
    </tr>
    <tr>
      <td><a href="https://www.cs.umd.edu/~metzler/index.html">Christopher Metzler</a><br />University of Maryland</td>
      <td><a href="https://sites.google.com/site/shirinjalali/home">Shirin Jalali</a><br />Rutgers University</td>
      <td><a href="https://ajiljalal.github.io/">Ajil Jalal</a><br />UC Berkeley</td>
    </tr>
    <tr>
      <td><img src="/assets/images/Paul.jpeg" alt="Paul Hand" width="225" /></td>
      <td><img src="/assets/images/Reinhard.jpg" alt="Reinhard Heckel" width="225" /></td>
      <td><img src="/assets/images/Jon.jpg" alt="Jon Tamir" width="225" /></td>
    </tr>
    <tr>
      <td><a href="http://khoury.northeastern.edu/home/hand/">Paul Hand</a><br />Northeastern University</td>
      <td><a href="http://www.reinhardheckel.com/">Reinhard Heckel</a><br />Technical University of Munich</td>
      <td><a href="https://users.ece.utexas.edu/~jtamir/">Jon Tamir</a><br />UT Austin</td>
    </tr>
    <tr>
      <td><img src="/assets/images/Arian.jpg" alt="Arian Maleki" width="225" /></td>
      <td><img src="/assets/images/Rich.jpg" alt="Rich Baraniuk" width="225" /></td>
      <td> </td>
    </tr>
    <tr>
      <td><a href="https://sites.google.com/site/malekiarian/">Arian Maleki</a><br />Columbia University</td>
      <td><a href="https://richb.rice.edu/">Richard Baraniuk</a><br />Rice University</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h1 id="call-for-papers-and-submission-instructions">Call for Papers and Submission Instructions</h1>

<p>We invite researchers to submit anonymous papers of up to 4 pages (excluding references and appendices) which will be considered for contributed workshop papers. No specific formatting is required. Authors are encouraged to use the <a href="https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles">NeurIPS style file</a>, but they may use any other style as long as it has standard font size (11pt) and margins (1in).</p>

<p>Submission at <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/Deep_Inverse">OpenReview</a> is open now until the submission deadline on October 6, 2023.</p>

<p>We welcome all submissions in the intersection of inverse problems and deep learning, including but not limited to submissions on the following topics:</p>

<ul>
  <li>
    <p>Fundamental approaches to address model uncertainty in learning-based solutions for inverse problems: Currently, the best DL-based solutions heavily rely on knowing the inverse system’s forward model and assume simple models of distortion (such as additive Gaussian noise). What algorithms and analysis techniques do we require for applications where we only have access to partial information about the system model?</p>
  </li>
  <li>
    <p>Diffusion models: Diffusion models have recently gained attention as powerful learned priors for solving inverse problems, due to their ability to model complex high-dimensional data across diverse modalities such as MRI, acoustics, graphs, proteins, etc. What are their benefits and limitations, and what are the optimal algorithms?</p>
  </li>
</ul>

<h1 id="important-dates">Important Dates</h1>

<ul>
  <li>Submission Deadline: October 6, 2023, 11:59 PM UTC.</li>
  <li>Notification of acceptance: October 20, 2023.
<!-- - SlidesLive upload for speaker videos: November 1, 2021 --></li>
  <li>Workshop: Saturday, December 16, 2023.</li>
</ul>

<h1 id="dual-submission-policy">Dual Submission Policy</h1>

<p>This is a non-archival workshop. Hence, you are allowed to resubmit to
other venues, as long as the other venue permits it.</p>

<p>If your main paper was accepted at NeurIPS and you wish to submit to
this workshop, we expect: (a) the two manuscripts to have sufficient
differences in experiments or theory, and (b) an author to physically
present accepted posters on the day of the workshop.</p>

<p>Please email <a href="mailto:deepinverse@gmail.com">deepinverse@gmail.com</a> with any questions.</p>



      <footer class="site-footer">
        <!--  -->
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
