<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Deep Learning and Inverse Problems | NeurIPS Workshop on Deep Learning and Inverse Problems</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Deep Learning and Inverse Problems" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="NeurIPS 2020 workshop, Friday December 11, 2020, Online" />
<meta property="og:description" content="NeurIPS 2020 workshop, Friday December 11, 2020, Online" />
<meta property="og:site_name" content="Deep Learning and Inverse Problems" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Learning and Inverse Problems" />
<script type="application/ld+json">
{"headline":"Deep Learning and Inverse Problems","description":"NeurIPS 2020 workshop, Friday December 11, 2020, Online","@type":"WebPage","url":"/2020.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Deep Learning and Inverse Problems</h1>
      <h2 class="project-tagline">NeurIPS 2020 workshop, Friday December 11, 2020, Online</h2>
      
      
        <a href="./2019.html" class="btn">2019</a>
        <a href="./2020.html" class="btn">2020</a>
        <a href="./index.html" class="btn">2021</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="workshop-material">Workshop Material</h1>

<p>All videos of the workshop are available at <a href="https://slideslive.com/neurips/workshop-on-deep-learning-and-inverse-problems">slideslive</a>.</p>

<p>All accepted papers are availabe at <a href="https://openreview.net/group?id=NeurIPS.cc/2020/Workshop/Deep_Inverse#accept-oral">openreview</a>.</p>

<h1 id="workshop-summary">Workshop Summary</h1>

<p>Learning-based methods, and in particular deep neural networks, have emerged as highly successful and universal tools for image and signal recovery and restoration. They achieve state-of-the-art results on tasks ranging from image denoising, image compression, and image reconstruction from few and noisy measurements. They are starting to be used in important imaging technologies, for example in GEs newest computational tomography scanners and in the newest generation of the iPhone.</p>

<p>The field has a range of theoretical and practical questions that remain unanswered. In particular, learning and neural network-based approaches often lack the guarantees of traditional physics-based methods. Further, while superior on average, learning-based methods can make drastic reconstruction errors, such as hallucinating a tumor in an MRI reconstruction or turning a pixelated picture of Obama into a white male.</p>

<p>This virtual workshop aims at bringing together theoreticians and practitioners in order to chart out recent advances and discuss new directions in deep neural network-based approaches for solving inverse problems in the imaging sciences and beyond.</p>

<h1 id="schedule">Schedule</h1>

<p>The workshop schedule is aligned with 7:30 AM to 4 PM PT; please see this <a href="https://www.timeanddate.com/worldclock/fixedtime.html?msg=Deep+Inverse+Workshop&amp;iso=20201211T0730&amp;p1=256&amp;ah=8&amp;am=30">converter</a> for conversion to your specific time zone.</p>

<p>The workshop is livestreamed at <a href="https://neurips.cc/virtual/2020/protected/workshop_16113.html">the NeurIPS workshop webpage</a>. To access this website you need to register for the conference at <a href="https://neurips.cc/">https://neurips.cc/</a> and be logged in.</p>

<p>The videos of the talk can also be previewed as of now through <a href="https://neurips.cc/virtual/2020/protected/workshop_16113.html">the NeurIPS workshop webpage</a>. The livestream will play the videos, and each of the talks is followed by a live discussion. You can attend the live discussion either through the live stream or you can join the zoom session. Questions should be asked through rocketchat, and a co-organizer will moderate.</p>

<p>During the designated discussion sessions you can talk to the respective speakers in gather.town. To get the gather.town link, visit the official NeurIPS schedule at <a href="https://neurips.cc/virtual/2020/protected/workshop_16113.html">the NeurIPS workshop webpage</a>.</p>

<p>The poster session is also on gather.town. The gather.town link is accessible through the <a href="https://neurips.cc/virtual/2020/protected/workshop_16113.html">the NeurIPS workshop webpage</a>.</p>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>7:30</td>
      <td>Newcomer presentation</td>
    </tr>
    <tr>
      <td>7:55</td>
      <td>Opening Remarks</td>
    </tr>
    <tr>
      <td>8:00</td>
      <td><a href="http://sites.skoltech.ru/compvision/members/vilem/">Victor Lempitsky</a> (Skoltech): Generative Models for Landscapes and Avatars</td>
    </tr>
    <tr>
      <td>8:30</td>
      <td><a href="https://www.tugraz.at/institute/icg/research/team-pock/people/pock/">Thomas Pock</a> (TU Graz): Variational Networks</td>
    </tr>
    <tr>
      <td>9:00</td>
      <td>Contributed talk 1: Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala, John M. Pauly: Risk Quantification in Deep MRI Reconstruction</td>
    </tr>
    <tr>
      <td>9:15</td>
      <td>Contributed talk 2: Sungmin Cha, Taeeon Park, Byeongjoon Kim, Jongduk Baek, Taesup Moon: GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images</td>
    </tr>
    <tr>
      <td>9:30</td>
      <td>Break and Discussion with Victor Lempinsky, Thomas Pock, and Erich Kobler</td>
    </tr>
    <tr>
      <td>10:00</td>
      <td><a href="https://voices.uchicago.edu/willett/">Rebecca Willett</a> (University of Chicago): Model Adaptation for Inverse Problems in Imaging</td>
    </tr>
    <tr>
      <td>10:30</td>
      <td><a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a> (Stanford): Generative Modeling via Denoising</td>
    </tr>
    <tr>
      <td>11:00</td>
      <td>Contributed talk 3: Ajil Jalal, Sushrut Karmalkar, Alex Dimakis, Eric Price: Compressed Sensing with Approximate Priors via Conditional Resampling</td>
    </tr>
    <tr>
      <td>11:15</td>
      <td>Chris Metzler: Approximate Message Passing (AMP) Algorithms for Computational Imaging</td>
    </tr>
    <tr>
      <td>11:30</td>
      <td>Discussion with Rebecca Willett and Stefano Emron</td>
    </tr>
    <tr>
      <td>12:00</td>
      <td>Break</td>
    </tr>
    <tr>
      <td>1:00</td>
      <td>Poster session</td>
    </tr>
    <tr>
      <td>2:00</td>
      <td><a href="https://sites.google.com/view/milanfarhome/">Peyman Milanfar</a> (Google) - Denoising as Building Block Theory and Applications</td>
    </tr>
    <tr>
      <td>2:30</td>
      <td><a href="https://www.oden.utexas.edu/people/1143/">Rachel Ward</a> (UT Austin)</td>
    </tr>
    <tr>
      <td>3:00</td>
      <td><a href="http://larryzitnick.org/">Larry Zitnick</a> (Facebook AI Reseach) - fastMRI</td>
    </tr>
    <tr>
      <td>3:30</td>
      <td>Discussion with Peyman Milanfar, Rachel Ward, and Larry Zitnick</td>
    </tr>
    <tr>
      <td>4:00</td>
      <td>End of official program</td>
    </tr>
  </tbody>
</table>

<h1 id="accepted-papers">Accepted papers</h1>

<p>All accepted papers are available <a href="https://openreview.net/group?id=NeurIPS.cc/2020/Workshop/Deep_Inverse#accept-poster">on openreview</a>:</p>

<ol>
  <li>
    <p><a href="https://openreview.net/forum?id=qMIW5nuCjwL">Approximate Probabilistic Inference with Composed Flows</a><br />
Jay Whang, Erik Lindgren, Alex Dimakis</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=P-0ae-EbP8">Bayesian Inference in Physics-Driven Problems with Adversarial Priors</a><br />
Dhruv V Patel, Deep Ray, Harisankar Ramaswamy, Assad Oberai</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=8ozSD4Oymw">Compressed Sensing with Approximate Priors via Conditional Resampling</a><br />
Ajil Jalal, Sushrut Karmalkar, Alex Dimakis, Eric Price</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=8R5nxDkbwE">Compressed Sensing with Invertible Generative Models and Dependent Noise</a><br />
Jay Whang, Qi Lei, Alex Dimakis</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=fuB3vZdTh0">Deep Learning for Plasma Tomography in Nuclear Fusion</a><br />
Diogo R. Ferreira, Pedro J. Carvalho</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=gv4I5IfJHP">Deep Learning Initialized Phase Retrieval</a><br />
Raunak Manekar, Zhong Zhuang, Kshitij Tayal, Vipin Kumar, Ju Sun</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=GpwoGZNeUC">Denoising Score-Matching for Uncertainty Quantification in Inverse Problems</a><br />
Zaccharie Ramzi, Benjamin Remy, Francois Lanusse, Jean-Luc Starck, Philippe Ciuciu</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=79TQKu3bXd">GAN2GAN: Generative Noise Learning for Blind Denoising with Single Noisy Images</a><br />
Sungmin Cha, Taeeon Park, Byeongjoon Kim, Jongduk Baek, Taesup Moon</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=5gs_SxdLozM">Generative Tomography Reconstruction</a><br />
Matteo Ronchetti, Davide Bacciu</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=s2EucjZ6d2s">Generator Surgery for Compressed Sensing</a><br />
Jung Yeon Park, Niklas Smedemark-Margulies, Max Daniels, Rose Yu, Jan-Willem van de Meent, Paul Hand</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=232ut-SyVR">Intermediate Layer Optimization for Inverse Problems using Deep Generative Models</a><br />
Joseph Dean, Giannis Daras, Alex Dimakis</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=lUgF584nOSY">Learning Spectral Regularizations for Linear Inverse Problems</a><br />
Hartmut Bauermeister, Martin Burger, Michael Moeller</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=1AOReNkDmh_">Learning to Sample MRI via Variational Information Maximization</a><br />
Cagan Alkan, Morteza Mardani, Shreyas Vasanawala, John M. Pauly</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=SiovfW3wF1e">Likelihood-Free Inference with Deep Gaussian Processes</a><br />
Alexander Aushev, Henri Pesonen, Markus Heinonen, Jukka Corander, Samuel Kaski</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=iUGcSYdJogv">Quantifying Sources of Uncertainty in Deep Learning-Based Image Reconstruction</a><br />
Riccardo Barbano, Zeljko Kereta, Chen Zhang, Andreas Hauptmann, Simon Arridge, Bangti Jin</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=ccBn_ZIuRHT">Risk Quantification in Deep MRI Reconstruction</a><br />
Vineet Edupuganti, Morteza Mardani, Shreyas Vasanawala, John M. Pauly</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=RLN7K4U3UST">Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser</a><br />
Zahra Kadkhodaie, Eero Peter Simoncelli</p>
  </li>
  <li>
    <p><a href="https://openreview.net/pdf?id=ysONncbu1z">Towards Neurally Augmented ALISTA</a><br />
Freya Behrens, Jonathan Sauder, Peter Jung</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=lWLYCQmtvW">Uncertainty-Driven Adaptive Sampling via GANs</a><br />
Thomas Sanchez, Igor Krawczuk, Zhaodong Sun, Volkan Cevher</p>
  </li>
  <li>
    <p><a href="https://openreview.net/forum?id=oyhGIytV1S">Unlocking Inverse Problems Using Deep Learning: Breaking Symmetries in Phase Retrieval</a><br />
Kshitij Tayal, Chieh-Hsin Lai, Raunak Manekar, Zhong Zhuang, Vipin Kumar, Ju Sun</p>
  </li>
</ol>

<h1 id="call-for-papers-and-submission-instructions">Call for Papers and Submission Instructions</h1>

<p>We invite researchers to submit anonymous papers of up to 4 pages (excluding references and appendices) which will be considered for contributed workshop papers. No specific formatting is required. Authors are encouraged to use the <a href="https://deep-inverse.org/downloads/NeurIPSDeepInverseStyle.zip">workshop edition of the NeurIPS style file</a>, but they may use any other style as long as it has standard font size (11pt) and margins (1in). The paper can have an appendix of unlimited lenght.</p>

<p>Submission at <a href="https://openreview.net/group?id=NeurIPS.cc/2020/Workshop/Deep_Inverse">OpenReview</a> will be open from Sep. 1 until the submission deadline on October 9, 2020.</p>

<p>We welcome all submission in the intersection of inverse problems and deep learning including contributions related to robustness and biases, neural network architectures, regularization, optimization methods, datasets, theoretical foundations (including rigorous recovery guarantees, provable convergence, and bounds on representation errors),  untrained methods, generative models, end-to-end methods, and applications in imaging, time series, and beyond. We especially encourage submissions related to the following questions:</p>

<ul>
  <li>
    <p>Deep learning based approaches can make drastic reconstruction errors and may introduce biases. How common are such issues, can such reconstruction difficulties be alleviated, and if yes, how?</p>
  </li>
  <li>
    <p>Deep learning based approaches often lack the guarantees of the traditional physics based methods. What theoretical results are necessary and possible?</p>
  </li>
  <li>
    <p>Untrained neural networks such as the deep image prior have shown that neural networks alone, without any learning, can give excellent reconstruction performance. How important is training on the target distribution for imaging performance and is it possible to achieve state-of-the art performance without training?</p>
  </li>
</ul>

<h1 id="important-dates">Important Dates</h1>
<ul>
  <li>Submit the final version and poster by <strong>December 1, 2020</strong> at <a href="https://openreview.net/group?id=NeurIPS.cc/2020/Workshop/Deep_Inverse">OpenReview</a></li>
  <li>Submission Deadline: Extended to <strong>October 12, 2020</strong> (this is the final deadline).</li>
  <li>Notification: October 23, 2020.</li>
  <li>Workshop: Friday December 11</li>
</ul>

<h1 id="organizers">Organizers</h1>
<ul>
  <li><a href="http://www.reinhardheckel.com/">Reinhard Heckel</a> (TUM)</li>
  <li><a href="http://khoury.northeastern.edu/home/hand/">Paul Hand</a> (Northeastern)</li>
  <li><a href="https://www.cs.umd.edu/~sfeizi/">Soheil Feizi</a> (UMD)</li>
  <li><a href="http://artax.karlin.mff.cuni.cz/~zdebl9am/">Lenka Zdeborova</a> (CEA/SACLAY)</li>
  <li><a href="http://richb.rice.edu/">Richard Baraniuk</a> (Rice University)</li>
</ul>

<p>Please email <a href="mailto:deepinverse@gmail.com">deepinverse@gmail.com</a> with any questions.</p>



      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>
